{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the notebook implementing basic LSTM networks for toxic classification with tensorflow. (refer to DL-ND sentiment_RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove.6B.100d.txt.zip\n",
      "glove6b100dtxt\n",
      "sample_submission.csv\n",
      "sample_submission.csv.zip\n",
      "test.csv\n",
      "test.csv.zip\n",
      "train.csv\n",
      "train.csv.zip\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(159571, 8)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../input/train.csv\")\n",
    "test_df = pd.read_csv(\"../input/test.csv\")\n",
    "\n",
    "print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(map(len, train_df.comment_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_list = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "labels = train_df[class_list].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 6)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's remove all punctuation. Then get all the text without the newlines and split it into individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_string(sentence):\n",
    "    text = \"\".join([c for c in sentence if c not in punctuation])\n",
    "    sens = \" \".join(text.split('\\n'))\n",
    "    words = sens.split(\" \")\n",
    "    \n",
    "    return [word.lower() for word in words if word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_sentences_train = train_df[\"comment_text\"].apply(clean_string).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding lookup requires that we pass in integers to our network. The easiest way to do this is to create dictionaries that map the words in the vocabulary to integers. Then we can convert each of our reviews into integers so they can be passed into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from functools import reduce\n",
    "\n",
    "#words = reduce(lambda x,y: x+y, list(list_sentences_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "#pickle.dump( words, open( \"words.pkl\", \"wb\" ) )\n",
    "words = pickle.load(open(\"words.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counts = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = sorted(counts, key=counts.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_tokenized_train = [[vocab_to_int[word] for word in sentence] for sentence in list_sentences_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ ['explanation', 'why', 'the', 'edits', 'made', 'under', 'my', 'username', 'hardcore', 'metallica', 'fan', 'were', 'reverted', 'they', 'werent', 'vandalisms', 'just', 'closure', 'on', 'some', 'gas', 'after', 'i', 'voted', 'at', 'new', 'york', 'dolls', 'fac', 'and', 'please', 'dont', 'remove', 'the', 'template', 'from', 'the', 'talk', 'page', 'since', 'im', 'retired', 'now892053827'],\n",
       "       ['daww', 'he', 'matches', 'this', 'background', 'colour', 'im', 'seemingly', 'stuck', 'with', 'thanks', 'talk', '2151', 'january', '11', '2016', 'utc'],\n",
       "       ['hey', 'man', 'im', 'really', 'not', 'trying', 'to', 'edit', 'war', 'its', 'just', 'that', 'this', 'guy', 'is', 'constantly', 'removing', 'relevant', 'information', 'and', 'talking', 'to', 'me', 'through', 'edits', 'instead', 'of', 'my', 'talk', 'page', 'he', 'seems', 'to', 'care', 'more', 'about', 'the', 'formatting', 'than', 'the', 'actual', 'info'],\n",
       "       ...,\n",
       "       ['spitzer', 'umm', 'theres', 'no', 'actual', 'article', 'for', 'prostitution', 'ring', 'crunch', 'captain'],\n",
       "       ['and', 'it', 'looks', 'like', 'it', 'was', 'actually', 'you', 'who', 'put', 'on', 'the', 'speedy', 'to', 'have', 'the', 'first', 'version', 'deleted', 'now', 'that', 'i', 'look', 'at', 'it'],\n",
       "       ['and', 'i', 'really', 'dont', 'think', 'you', 'understand', 'i', 'came', 'here', 'and', 'my', 'idea', 'was', 'bad', 'right', 'away', 'what', 'kind', 'of', 'community', 'goes', 'you', 'have', 'bad', 'ideas', 'go', 'away', 'instead', 'of', 'helping', 'rewrite', 'them']], dtype=object)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##  Feature for sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_len = 300\n",
    "features = np.zeros((len(list_tokenized_train), max_len), dtype=int)\n",
    "for i, row in enumerate(list_tokenized_train):\n",
    "    features[i, -len(row):] = np.array(row)[:max_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset split training, validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(127656, 300) \n",
      "Validation set: \t(15957, 300) \n",
      "Test set: \t\t(15958, 300)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "split_idx = int(len(features)*0.8)\n",
    "train_x, val_x = features[:split_idx], features[split_idx:]\n",
    "train_y, val_y = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(val_x)*0.5)\n",
    "val_x, test_x = val_x[:test_idx], val_x[test_idx:]\n",
    "val_y, test_y = val_y[:test_idx], val_y[test_idx:]\n",
    "\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build computation graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we'll build the graph. First up, defining the hyperparameters.\n",
    "\n",
    "* `lstm_size`: Number of units in the hidden layers in the LSTM cells. Usually larger is better performance wise. Common values are 128, 256, 512, etc.\n",
    "* `lstm_layers`: Number of LSTM layers in the network. I'd start with 1, then add more if I'm underfitting.\n",
    "* `batch_size`: The number of reviews to feed the network in one training pass. Typically this should be set as high as you can go without running out of memory.\n",
    "* `learning_rate`: Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lstm_size = 256\n",
    "lstm_layers = 1\n",
    "batch_size = 500\n",
    "learning_rate = 0.00002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the network itself, we'll be passing in our 200 element long review vectors. Each batch will be `batch_size` vectors. We'll also be using dropout on the LSTM layer, so we'll make a placeholder for the keep probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    \n",
    "    inputs_ = tf.placeholder(tf.int32, [None, None], name=\"inputs\")\n",
    "    labels_ = tf.placeholder(tf.int32, [None, 6], name=\"labels\")  \n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the embedding lookup matrix as a `tf.Variable`. Use that embedding matrix to get the embedded vectors to pass to the LSTM cell with [`tf.nn.embedding_lookup`](https://www.tensorflow.org/api_docs/python/tf/nn/embedding_lookup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed_size = 300\n",
    "n_words = len(vocab_to_int)\n",
    "with graph.as_default():\n",
    "    embedding = tf.Variable(tf.random_uniform((n_words, embed_size), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a basic LSTM cell for the graph, you'll want to use tf.contrib.rnn.BasicLSTMCell. Looking at the function documentation: tf.contrib.rnn.BasicLSTMCell(num_units, forget_bias=1.0, input_size=None, state_is_tuple=True, activation=<function tanh at 0x109f1ef28>)\n",
    "you can see it takes a parameter called num_units, the number of units in the cell, called lstm_size in this code. So then, you can write something like \n",
    "\n",
    "* lstm = tf.contrib.rnn.BasicLSTMCell(num_units)\n",
    "to create an LSTM cell with num_units.\n",
    "\n",
    "Next, you can add dropout to the cell with tf.contrib.rnn.DropoutWrapper. This just wraps the cell in another cell, but with dropout added to the inputs and/or outputs. It's a really convenient way to make your network better with almost no effort! So you'd do something like\n",
    "* drop = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
    "\n",
    "Most of the time, you're network will have better performance with more layers. That's sort of the magic of deep learning, adding more layers allows the network to learn really complex relationships. Again, there is a simple way to create multiple layers of LSTM cells with tf.contrib.rnn.MultiRNNCell:\n",
    "* cell = tf.contrib.rnn.MultiRNNCell([drop] * lstm_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    #lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    #dropout = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob = keep_prob)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(\n",
    "        [tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.BasicLSTMCell(lstm_size), \n",
    "                                                                      output_keep_prob = keep_prob) for _ in range(lstm_layers)])\n",
    "    \n",
    "    # Getting an initial state of all zeros\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use tf.nn.dynamic_rnn to add the forward pass through the RNN. Remember that we're actually passing in vectors from the embedding layer, embed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed, initial_state = initial_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    logits = tf.contrib.layers.fully_connected(outputs[:, -1], 6)\n",
    "    cost = tf.losses.sigmoid_cross_entropy(labels_, logits)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    correct_pred = tf.equal(tf.cast(tf.argmax(logits), tf.int32), labels_)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get bactch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(x, y, batch_size=100):\n",
    "    \n",
    "    n_batches = len(x)//batch_size\n",
    "    x, y = x[:n_batches*batch_size], y[:n_batches*batch_size]\n",
    "    for ii in range(0, len(x), batch_size):\n",
    "        yield x[ii:ii+batch_size], y[ii:ii+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x,y in get_batches(train_x, train_y, batch_size):\n",
    "    print(x.shape,y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0/10 Iteration: 5 Train loss: 0.7243062853813171\n",
      "Epoch: 0/10 Iteration: 10 Train loss: 0.7158084511756897\n",
      "Epoch: 0/10 Iteration: 15 Train loss: 0.7078432440757751\n",
      "Epoch: 0/10 Iteration: 20 Train loss: 0.7034446597099304\n",
      "Epoch: 0/10 Iteration: 25 Train loss: 0.7004823684692383\n",
      "Val acc: 0.005\n",
      "Epoch: 0/10 Iteration: 30 Train loss: 0.6967126727104187\n",
      "Epoch: 0/10 Iteration: 35 Train loss: 0.6952726244926453\n",
      "Epoch: 0/10 Iteration: 40 Train loss: 0.6944915652275085\n",
      "Epoch: 0/10 Iteration: 45 Train loss: 0.6940205097198486\n",
      "Epoch: 0/10 Iteration: 50 Train loss: 0.6935234665870667\n",
      "Val acc: 0.389\n",
      "Epoch: 0/10 Iteration: 55 Train loss: 0.6934249401092529\n",
      "Epoch: 0/10 Iteration: 60 Train loss: 0.6932600736618042\n",
      "Epoch: 0/10 Iteration: 65 Train loss: 0.693517804145813\n",
      "Epoch: 0/10 Iteration: 70 Train loss: 0.6932862997055054\n",
      "Epoch: 0/10 Iteration: 75 Train loss: 0.6931747794151306\n",
      "Val acc: 0.706\n",
      "Epoch: 0/10 Iteration: 80 Train loss: 0.6932008266448975\n",
      "Epoch: 0/10 Iteration: 85 Train loss: 0.6932100653648376\n",
      "Epoch: 0/10 Iteration: 90 Train loss: 0.6931633949279785\n",
      "Epoch: 0/10 Iteration: 95 Train loss: 0.6932317614555359\n",
      "Epoch: 0/10 Iteration: 100 Train loss: 0.6931758522987366\n",
      "Val acc: 0.773\n",
      "Epoch: 0/10 Iteration: 105 Train loss: 0.6933184266090393\n",
      "Epoch: 0/10 Iteration: 110 Train loss: 0.6931471228599548\n",
      "Epoch: 0/10 Iteration: 115 Train loss: 0.6929179430007935\n",
      "Epoch: 0/10 Iteration: 120 Train loss: 0.6931471228599548\n",
      "Epoch: 0/10 Iteration: 125 Train loss: 0.6931471228599548\n",
      "Val acc: 0.814\n",
      "Epoch: 0/10 Iteration: 130 Train loss: 0.6932555437088013\n",
      "Epoch: 0/10 Iteration: 135 Train loss: 0.6930702328681946\n",
      "Epoch: 0/10 Iteration: 140 Train loss: 0.693347156047821\n",
      "Epoch: 0/10 Iteration: 145 Train loss: 0.6932530999183655\n",
      "Epoch: 0/10 Iteration: 150 Train loss: 0.6931756138801575\n",
      "Val acc: 0.820\n",
      "Epoch: 0/10 Iteration: 155 Train loss: 0.6925961971282959\n",
      "Epoch: 0/10 Iteration: 160 Train loss: 0.6932640671730042\n",
      "Epoch: 0/10 Iteration: 165 Train loss: 0.6929891109466553\n",
      "Epoch: 0/10 Iteration: 170 Train loss: 0.6931471228599548\n",
      "Epoch: 0/10 Iteration: 175 Train loss: 0.6931537389755249\n",
      "Val acc: 0.841\n",
      "Epoch: 0/10 Iteration: 180 Train loss: 0.6931524276733398\n",
      "Epoch: 0/10 Iteration: 185 Train loss: 0.6931471228599548\n",
      "Epoch: 0/10 Iteration: 190 Train loss: 0.6931573152542114\n",
      "Epoch: 0/10 Iteration: 195 Train loss: 0.6931471228599548\n",
      "Epoch: 0/10 Iteration: 200 Train loss: 0.6931784749031067\n",
      "Val acc: 0.846\n",
      "Epoch: 0/10 Iteration: 205 Train loss: 0.6931471228599548\n",
      "Epoch: 0/10 Iteration: 210 Train loss: 0.6931688785552979\n",
      "Epoch: 0/10 Iteration: 215 Train loss: 0.6931471228599548\n",
      "Epoch: 0/10 Iteration: 220 Train loss: 0.6931471228599548\n",
      "Epoch: 0/10 Iteration: 225 Train loss: 0.6931471228599548\n",
      "Val acc: 0.846\n",
      "Epoch: 0/10 Iteration: 230 Train loss: 0.6931471228599548\n",
      "Epoch: 0/10 Iteration: 235 Train loss: 0.6931471228599548\n",
      "Epoch: 0/10 Iteration: 240 Train loss: 0.6931471228599548\n",
      "Epoch: 0/10 Iteration: 245 Train loss: 0.693321704864502\n",
      "Epoch: 0/10 Iteration: 250 Train loss: 0.6931478381156921\n",
      "Val acc: 0.841\n",
      "Epoch: 0/10 Iteration: 255 Train loss: 0.6931471228599548\n",
      "Epoch: 1/10 Iteration: 260 Train loss: 0.6930956244468689\n",
      "Epoch: 1/10 Iteration: 265 Train loss: 0.6931479573249817\n",
      "Epoch: 1/10 Iteration: 270 Train loss: 0.6931471228599548\n",
      "Epoch: 1/10 Iteration: 275 Train loss: 0.6931471228599548\n",
      "Val acc: 0.841\n",
      "Epoch: 1/10 Iteration: 280 Train loss: 0.6931695342063904\n",
      "Epoch: 1/10 Iteration: 285 Train loss: 0.6931471228599548\n",
      "Epoch: 1/10 Iteration: 290 Train loss: 0.6931586861610413\n",
      "Epoch: 1/10 Iteration: 295 Train loss: 0.693158745765686\n",
      "Epoch: 1/10 Iteration: 300 Train loss: 0.6931471228599548\n",
      "Val acc: 0.846\n",
      "Epoch: 1/10 Iteration: 305 Train loss: 0.6931471228599548\n",
      "Epoch: 1/10 Iteration: 310 Train loss: 0.6931473016738892\n",
      "Epoch: 1/10 Iteration: 315 Train loss: 0.6931506991386414\n",
      "Epoch: 1/10 Iteration: 320 Train loss: 0.6933022737503052\n",
      "Epoch: 1/10 Iteration: 325 Train loss: 0.6931471228599548\n",
      "Val acc: 0.851\n",
      "Epoch: 1/10 Iteration: 330 Train loss: 0.6931504011154175\n",
      "Epoch: 1/10 Iteration: 335 Train loss: 0.6931471228599548\n",
      "Epoch: 1/10 Iteration: 340 Train loss: 0.6931899189949036\n",
      "Epoch: 1/10 Iteration: 345 Train loss: 0.6931471228599548\n",
      "Epoch: 1/10 Iteration: 350 Train loss: 0.6931471228599548\n",
      "Val acc: 0.851\n",
      "Epoch: 1/10 Iteration: 355 Train loss: 0.6931471228599548\n",
      "Epoch: 1/10 Iteration: 360 Train loss: 0.6931471228599548\n",
      "Epoch: 1/10 Iteration: 365 Train loss: 0.6931471228599548\n",
      "Epoch: 1/10 Iteration: 370 Train loss: 0.6929683685302734\n",
      "Epoch: 1/10 Iteration: 375 Train loss: 0.6931716203689575\n",
      "Val acc: 0.851\n",
      "Epoch: 1/10 Iteration: 380 Train loss: 0.6931471228599548\n",
      "Epoch: 1/10 Iteration: 385 Train loss: 0.6932560801506042\n",
      "Epoch: 1/10 Iteration: 390 Train loss: 0.6928783655166626\n",
      "Epoch: 1/10 Iteration: 395 Train loss: 0.6931471228599548\n",
      "Epoch: 1/10 Iteration: 400 Train loss: 0.6931471228599548\n",
      "Val acc: 0.856\n",
      "Epoch: 1/10 Iteration: 405 Train loss: 0.6931471228599548\n",
      "Epoch: 1/10 Iteration: 410 Train loss: 0.6924269199371338\n",
      "Epoch: 1/10 Iteration: 415 Train loss: 0.6932348012924194\n",
      "Epoch: 1/10 Iteration: 420 Train loss: 0.692960262298584\n",
      "Epoch: 1/10 Iteration: 425 Train loss: 0.6931471228599548\n",
      "Val acc: 0.861\n",
      "Epoch: 1/10 Iteration: 430 Train loss: 0.6931360960006714\n",
      "Epoch: 1/10 Iteration: 435 Train loss: 0.6931471228599548\n",
      "Epoch: 1/10 Iteration: 440 Train loss: 0.6931471228599548\n",
      "Epoch: 1/10 Iteration: 445 Train loss: 0.6931471228599548\n",
      "Epoch: 1/10 Iteration: 450 Train loss: 0.6931471228599548\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    iteration = 1\n",
    "    for e in range(epochs):\n",
    "        state = sess.run(initial_state)\n",
    "        \n",
    "        for ii, (x, y) in enumerate(get_batches(train_x, train_y, batch_size), 1):\n",
    "            feed = {inputs_: x,\n",
    "                    labels_: y,\n",
    "                    keep_prob: 0.5,\n",
    "                    initial_state: state}\n",
    "            loss, state, _ = sess.run([cost, final_state, optimizer], feed_dict=feed)\n",
    "            \n",
    "            if iteration%5==0:\n",
    "                print(\"Epoch: {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Train loss: {}\".format(loss))\n",
    "\n",
    "            if iteration%25==0:\n",
    "                val_acc = []\n",
    "                val_state = sess.run(cell.zero_state(batch_size, tf.float32))\n",
    "                for x, y in get_batches(val_x, val_y, batch_size):\n",
    "                    feed = {inputs_: x,\n",
    "                            labels_: y,\n",
    "                            keep_prob: 1,\n",
    "                            initial_state: val_state}\n",
    "                    batch_acc, val_state = sess.run([accuracy, final_state], feed_dict=feed)\n",
    "                    val_acc.append(batch_acc)\n",
    "                print(\"Val acc: {:.3f}\".format(np.mean(val_acc)))\n",
    "            iteration +=1\n",
    "    saver.save(sess, \"checkpoints/basic_lstm_tf.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
