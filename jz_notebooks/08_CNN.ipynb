{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wszjz\\AppData\\Local\\conda\\conda\\envs\\tfgpu\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten,Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model\n",
    "from keras import regularizers\n",
    "\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '../input/'\n",
    "EMBEDDING_FILE = path + 'glove.6B/glove.6B.300d.txt'\n",
    "TRAIN_DATA_FILE = path + 'train.csv'\n",
    "TEST_DATA_FILE = path + 'test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EXTRAIN_DATA_FILE = path\n",
    "EXTRA_DAT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test_df = pd.read_csv(TEST_DATA_FILE)\n",
    "\n",
    "if EXTRA_DAT:\n",
    "    extra_df = pd.read_csv(EXTRAIN_DATA_FILE)\n",
    "    \n",
    "list_sentences_train = train_df[\"comment_text\"].fillna(\"_na_\").values\n",
    "\n",
    "class_list = [\"toxic\", \"severe_toxic\", \"obscene\", \n",
    "              \"threat\", \"insult\", \"identity_hate\"]\n",
    "labels = train_df[class_list].values\n",
    "\n",
    "list_sentences_test = test_df[\"comment_text\"].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### basic config param\n",
    "embed_size = 300\n",
    "max_features = 20000\n",
    "maxlen = 800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "\n",
    "features_train = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "features_test = pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "embeddings_index = dict(\n",
    "    get_coefs(*o.strip().split()\n",
    "             ) for o in open(EMBEDDING_FILE, encoding=\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate random number matrix as place holder\n",
    "word_index = tokenizer.word_index\n",
    "nb_words = max(max_features, len(word_index))\n",
    "#embedding_matrix = np.random.normal(emb_mean, emb_std, (max_features, embed_size))\n",
    "embedding_matrix = np.zeros((max_features, embed_size))\n",
    "\n",
    "# insert glove word vectors into the embedding matrix accoding to word index\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 800)               0         \n",
      "_________________________________________________________________\n",
      "embedding_8 (Embedding)      (None, 800, 300)          6000000   \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 796, 512)          768512    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_22 (MaxPooling (None, 159, 512)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 155, 256)          655616    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_23 (MaxPooling (None, 31, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 27, 128)           163968    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_24 (MaxPooling (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 640)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               82048     \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 7,670,918\n",
      "Trainable params: 7,670,918\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print('Training model.')\n",
    "\n",
    "# train a 1D convnet with global maxpooling\n",
    "sequence_input = Input(shape=(maxlen,), dtype='int32')\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(max_features,\n",
    "                            embed_size,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=maxlen,\n",
    "                            trainable=True)\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "x = Conv1D(512, 5, activation='relu')(embedded_sequences)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(256, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Conv1D(128, 5, activation='relu')(x)\n",
    "x = MaxPooling1D(5)(x)\n",
    "x = Flatten()(x)\n",
    "#x = Dropout(0.8)(x)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "#x = Dropout(0.8)(x)\n",
    "preds = Dense(6, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(sequence_input, preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exp_decay = lambda init, fin, steps: (init/fin)**(1/(steps-1)) - 1\n",
    "steps = int(len(features_train)/batch_size) * epochs\n",
    "lr_init, lr_fin = 0.001, 0.00005\n",
    "lr_decay = exp_decay(lr_init, lr_fin, steps)\n",
    "K.set_value(model.optimizer.lr, lr_init)\n",
    "K.set_value(model.optimizer.decay, lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/4\n",
      "143613/143613 [==============================] - 351s - loss: 0.1304 - acc: 0.9648 - val_loss: 0.1250 - val_acc: 0.9649\n",
      "Epoch 2/4\n",
      "143613/143613 [==============================] - 349s - loss: 0.1227 - acc: 0.9657 - val_loss: 0.1251 - val_acc: 0.9649\n",
      "Epoch 3/4\n",
      "143613/143613 [==============================] - 349s - loss: 0.1196 - acc: 0.9663 - val_loss: 0.1262 - val_acc: 0.9651\n",
      "Epoch 4/4\n",
      "143613/143613 [==============================] - 349s - loss: 0.1172 - acc: 0.9670 - val_loss: 0.1343 - val_acc: 0.9652\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x17fd51c6e10>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(features_train, labels,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
